#--------------------------------------------------------------------
# Step1: IMPORT NECESSARY LIBRARIES
#--------------------------------------------------------------------

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from scipy.stats import zscore
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

#--------------------------------------------------------------------
# Step2: IMPORT DATA
#--------------------------------------------------------------------
file_path = ('/Users/darienprall/Documents/GitHub/va_utilization/va_utilization_data.csv')
data = pd.read_csv(file_path)

#--------------------------------------------------------------------
# Step3: DEFINE & CLEAN DATA
# - 3.1: DEFINITIONS
# VETERANS WHO USED MULTIPLE PROGRAMS ARE COUNTED IN EACH PROGRAM BUT ONLY ONCE IN THE TOTAL
# THE SUM OF USERS IN EACH PROGRAM DOES NOT EQUAL TOTAL USERS
#
# State                                     The state of the feature data
# Total Veteran Population                  The total count of veterans in the given state
# Total VA Users: Veterans only             The total count of veterans utilizing the Veterans Affairs (does not include, active duty)
# VA Health Care                            The total count of veterans enrolled into the Veterans Affairs Health Care
# Disability Compensation OR Pension        The total count of veterans enrolled in disability compensation or pension
# VA Home Loan Guaranty                     The total count of veterans who utilized the Veterans Affairs Home Loan Guaranty
# VA Life Insurance                         The total count of veterans enrolled in the Veterans Affairs Life Insurance
# Education Benefits                        The total count of veterans utilizing educational benefits
# Memorial Benefits                         The total count of veterans utilizing memorial benefits
# Vocational Rehabilitation & Employment    The total count of veterans utilizing vocational rehabilitation & employment benefits
#
#--------------------------------------------------------------------

# - 3.2: CLEANING DATA
# - 3.2.1: Place all numerical categories in an array to loop through and convert
numerical_features = [
    'Total Veteran Population',
    'Total VA Users: Veterans only',
    'VA Health Care',
    'Disability Compensation OR Pension',
    'VA Home Loan Guaranty',
    'VA Life Insurance',
    'Education Benefits',
    'Memorial Benefits',
    'Vocational Rehabilitation & Employment'
]

# 3.2.2: Loop through numerical_features to remove commas, whitespace, tabs and convert to_numeric
for column in numerical_features:
    # Remove trailing and leading whitespace
    data[column] = data[column].str.strip()

    # Remove tabs and commas
    data[column] = data[column].replace({r'\t': '', r',': ''}, regex = True)

    # Convert to_numeric
    data[column] = pd.to_numeric(data[column], errors = 'coerce')

#print(data.head())

#--------------------------------------------------------------------
# Step4: SEARCH ALGORITHM DEMO (ANOMALY DETECTION)
# - In this example, I want to determine the top 5 states in which veterans are not using the VA
# - Here are my steps in solving this
# - 4.1: Calculate the utilization rate for each state (Utilization Rate = Total VA Users: Veterans only / Total Veteran Population)
# - 4.2: Apply a Z-score threshold of 2 or -2 to simplify the detection of anomalies 
# - 4.3: Sort the data by lowest utilization rate to determine the top 5 states
# - 4.4: Output the results
#--------------------------------------------------------------------

# - 4.1: Calculate Utilization Rate
data['Utilization Rate'] = data['Total VA Users: Veterans only'] / data['Total Veteran Population']
#print(data.head())

# - 4.2: Z-Scores with a threshold of 2
threshold = 2
data['Z-Score'] = zscore(data['Utilization Rate'])

# - 4.2.1: Anomaly Detection (any states > or < 2)
anomalies = data[data['Z-Score'].abs() > 2 ]
#print(anomalies)

# - 4.2.2: Filtering out anomalies
data_wo_anomalies = data[(data['Z-Score'] < threshold) & (data['Z-Score'] > -threshold)]
#print(data_wo_anomalies)


# - 4.3: Sorting the data
data_wo_anomalies_sorted = data_wo_anomalies.sort_values(by='Utilization Rate', ascending = True)
#print(data_wo_anomalies_sorted) 

# - 4.3.1: Determining Top 5 Lowest Utilization Rates
top_5_lowest_utilization_rates = data_wo_anomalies_sorted[['State', 'Utilization Rate']].head(5)

# - 4.4: Output
print("The top 5 states with the lowest utilization rates are as follows: ")
print("STATE\t\tUTILIZATION RATE")
for index, row in top_5_lowest_utilization_rates.iterrows():
    print(f"{row['State']}\t| {row['Utilization Rate']:.2f}")

#--------------------------------------------------------------------
# Step5: MACHINE LEARNING ALGORITHM DEMO
# - For the machine learning algorithm demo, I will be using KMeans clustering
# - I will identify groups of states based on their ['Utilization Rate'] to identify patterns on how states utilize VA Services
# - Here are my steps for solving this:
# - 5.1: Identify features for clustering
# - 5.2: Normalize the data with KMeans
# - 5.3: Apply Clustering (KMeans) with X clusters
# - 5.4: Show cluster centers
# - 5.5: Visualization (plot)
# - 5.6: Output the results
#--------------------------------------------------------------------

# - 5.1: Identify features for clustering
cluster_features = [
    'Total Veteran Population',
    'Total VA Users: Veterans only',
    'VA Health Care',
    'Disability Compensation OR Pension',
    'VA Home Loan Guaranty',
    'VA Life Insurance',
    'Education Benefits',
    'Memorial Benefits',
    'Vocational Rehabilitation & Employment'
]

# - 5.2: Normalization
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data[cluster_features])

# - 5.3: KMeans
kmeans = KMeans(n_clusters = 5, random_state = 42)
data['Clusters'] = kmeans.fit_predict(data_scaled)

# - 5.4 Show Clusters
print("Cluster Centers (average values of each cluster): ")
#print(kmeans.cluster_centers_)

# - 5.5: Vizualize
plt.figure(figsize=(10, 6))
plt.scatter(data['Total Veteran Population'], data['Total VA Users: Veterans only'], 
            c=data['Clusters'], cmap='viridis', s=100, edgecolors='k', alpha=0.7)
plt.xlabel('Total Veteran Population')
plt.ylabel('Total VA Users: Veterans only')
plt.title('KMeans Clustering of States based on VA Services Utilization')
plt.colorbar(label='Cluster')

plt.show()


# - 5.6: Output Results (will print once Figure 1 is closed)
print("\nStates with their assigned clusters:")
for index, row in data.iterrows():
    print(f"State: {row['State']} - Cluster: {row['Clusters']}")


#--------------------------------------------------------------------
# Step6: Markov Model Demo (Probabilistic Transitions)
# - The Markov model will be tricky to demonstrate as in needs sequential data
# - The current data I have is a single snapshot from 2016
# - However, I can show transitions between states based on their usage
# - I will show the transition probabilites of the Education Benefits state of low, medium, and high
# - 6.1: Define States (features)
# - 6.2: Create Transition Matrix
# - 6.3: Simulate Transitions
# - 6.4: Visualize the output
#--------------------------------------------------------------------

# - 6.1: Define States for Education Benefits (Low, Medium, High)

# - 6.1.1: Get the percentiles of Education Benefits, starting with low (33) and high (66)
low_percentile = np.percentile(data['Education Benefits'], 33)
high_percentile = np.percentile(data['Education Benefits'], 66)
#print(low_percentile) #4985.36
#print(high_percentile) #13239.12

# - 6.1.2: Define the function to calcuate percentiles dynamically
def categorize_usage(x, low_percentile, high_percentile):
    if x < low_percentile:
        return 'Low'
    elif x < high_percentile:
        return 'Medium'
    else:
        return 'High'

# - 6.1.4: Create Usage Feature
data['Education Benefits Usage'] = data['Education Benefits'].apply(categorize_usage, low_percentile=low_percentile, high_percentile=high_percentile)
#print(data)

# - 6.2: Creating the Transition Matrix

# - 6.2.1: Define each state
states = ['Low', 'Medium', 'High']

# - 6.2.1: Initialize the matrix
transition_matrix = pd.DataFrame(0, index = states, columns = states)

# - 6.2.2: Fill in the matrix
for i in range(len(data) -1 ):
    current_state = data.iloc[i]['Education Benefits Usage']
    next_state = data.iloc[i+1]['Education Benefits Usage']
    transition_matrix.loc[current_state, next_state] +=1

# - 6.2.3: Convert each count to a probability
transition_matrix = transition_matrix.div(transition_matrix.sum(axis = 1),axis = 0)

# - 6.3: Simulate Transitions
initial_state = 'Low'
steps = 10
current_state = initial_state
state_history = [current_state]

for _ in range(steps):
    transition_probabilities = transition_matrix.loc[current_state].values
    next_state = np.random.choice(states, p = transition_probabilities)
    state_history.append(next_state)
    current_state = next_state

# - 6.4: Output the results
print(f"Transition Matrix:\n{transition_matrix}")
print(f"\nSimulated States over {steps} steps:")
print(state_history)

# - 6.5: Visualize Results
plt.figure(figsize=(10, 6))
plt.plot(state_history, marker='o', linestyle='-', color='b')
plt.title(f"Markov Chain Simulation for Education Benefits Usage")
plt.xlabel('Time Steps')
plt.ylabel('State (Low, Medium, High)')
plt.xticks(range(steps + 1), range(steps + 1))
plt.yticks([0, 1, 2], states)
plt.grid(True)
plt.show()



